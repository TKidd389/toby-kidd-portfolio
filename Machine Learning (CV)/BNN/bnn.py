# -*- coding: utf-8 -*-
"""BNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zVUSCaxPg0lrLTa90BE8ovOjcTitABw7

# Setup

## Imports
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
#import tensorflow_datasets as tfds
import tensorflow_probability as tfp
import math
from sklearn.model_selection import train_test_split

"""## Hyperparameters"""

colab = False

numParams = 1

xImageRes, yImageRes = 128, 128

if colab == True:
  from google.colab import drive
  simulationFile = "shuffled_augmented_resized_Maps_Mtot_Nbody_SIMBA_LH_z=0.00.npy"
  parametersFile = "shuffled_augmented_resized_params_LH_Nbody_SIMBA.npy"
else:
  simulationFile = np.load("../../processed_data/128x128/shuffled_augmented_resized_Maps_Mtot_Nbody_SIMBA_LH_z=0.00.npy")
  parametersFile = np.load('../../processed_data/128x128/shuffled_augmented_resized_params_LH_Nbody_SIMBA.npy')

#simulationFile = "shuffled_resized_Maps_Mtot_Nbody_SIMBA_LH_z=0.00.npy"
#parametersFile = "shuffled_resized_params_LH_Nbody_SIMBA.npy"

percentTrain = 0.9

if colab == True:
  drive.mount('/content/drive')

"""# Create training and evaluation datasets"""

def createDatasets(filename, percent):
  if colab == True:
    fgrids = '/content/drive/MyDrive/ColabNotebooks/' + filename
    grids  = np.load(fgrids)#, mmap_mode='r')
  else:
    grids = simulationFile
  return grids

dataset = createDatasets(simulationFile, percentTrain)

dataset = np.expand_dims(dataset, axis=-1)

def getParamArrays(filename, param, percent):
  if colab == True:
    filepath = '/content/drive/MyDrive/ColabNotebooks/' + filename
    paramArray  = np.load(filepath)#, mmap_mode='r')
  else:
    paramArray = parametersFile
  return paramArray

omegaParamArray = getParamArrays(parametersFile, "Omega_m", percentTrain)
omegaParamArray = np.array(omegaParamArray, dtype=float)

labels = np.zeros((len(dataset), numParams))

minmax = np.zeros((numParams,2))

for i in range(numParams):
  # Calculate mean and standard deviation
  mean = np.mean(omegaParamArray[:,i])
  std_dev = np.std(omegaParamArray[:,i])

  # Standardize the array
  labels[:,i] = (omegaParamArray[:,i] - mean) / std_dev
  """
  y_min = np.min(omegaParamArray[:,i])
  minmax[i][0] = y_min
  y_max = np.max(omegaParamArray[:,i])
  minmax[i][1] = y_max
  labels[:,i] = (omegaParamArray[:,i] - y_min) / (y_max - y_min)
  """

trainDataset, testDataset, trainLabels, testLabels = train_test_split(dataset, labels, test_size=0.2, random_state=42)

"""# Compile, train, and evaluate the model"""

hidden_units = [128]  #[8, 8]
learning_rate = 0.001


def run_experiment(model, loss, train_dataset, train_labels, test_dataset, test_labels):

    model.compile(
        optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),
        loss=loss,
        metrics=[keras.metrics.RootMeanSquaredError()],
    )

    print("Start training the model...")
    model.fit(train_dataset, train_labels, epochs=num_epochs, validation_data=(test_dataset, test_labels))
    print("Model training finished.")
    _, rmse = model.evaluate(train_dataset, train_labels, verbose=0)
    print(f"Train RMSE: {round(rmse, 3)}")

    print("Evaluating model performance...")
    _, rmse = model.evaluate(test_dataset, test_labels, verbose=0)
    print(f"Test RMSE: {round(rmse, 3)}")

dataset_size = len(dataset)
batch_size = 32
train_size = int(dataset_size * 0.85)

num_epochs = 40
mse_loss = keras.losses.MeanSquaredError()

"""# Probabilistic Bayesian neural network"""

def create_probabilistic_bnn_model(train_size):
    hidden_units = [128]  # Example hidden units, adjust as needed

    inputs = layers.Input(shape=(xImageRes, yImageRes, 1))
    x = layers.Conv2D(32, (3, 3), padding='same')(inputs)
    x = layers.LeakyReLU()(x)
    x = layers.Conv2D(32, (3, 3), padding='same')(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.LeakyReLU()(x)
    x = layers.Conv2D(64, (3, 3), padding='same')(x)
    x = layers.LeakyReLU()(x)
    x = layers.Conv2D(64, (3, 3), padding='same')(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.LeakyReLU()(x)
    x = layers.Conv2D(128, (3, 3), padding='same')(x)
    x = layers.LeakyReLU()(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.LeakyReLU()(x)

    # Dense layers
    x = layers.Flatten()(x)
    x = layers.Dense(128, activation=tf.nn.leaky_relu)(x)
    x = layers.Dropout(0.2)(x)

    # Create hidden layers with weight uncertainty
    for units in hidden_units:
        x = tfp.layers.DenseVariational(
            units=units,
            make_prior_fn=prior,
            make_posterior_fn=posterior,
            kl_weight=1/train_size,
            activation='leaky_relu',
        )(x)

    # Output layer for probabilistic output
    distribution_params = layers.Dense(units=2)(x)
    outputs = tfp.layers.IndependentNormal(1)(distribution_params)

    model = keras.Model(inputs=inputs, outputs=outputs)
    model.summary()
    return model

def negative_loglikelihood(targets, estimated_distribution):
    return -estimated_distribution.log_prob(targets)


num_epochs = 1000
prob_bnn_model = create_probabilistic_bnn_model(len(trainDataset))
run_experiment(prob_bnn_model, negative_loglikelihood, trainDataset, trainLabels, testDataset, testLabels)

sample = 10
examples = testDataset[0:sample]
targets = testLabels[0:sample]

prediction_distribution = prob_bnn_model(examples)
prediction_mean = prediction_distribution.mean().numpy().tolist()
prediction_stdv = prediction_distribution.stddev().numpy()

# The 95% CI is computed as mean Â± (1.96 * stdv)
upper = (prediction_mean + (1.96 * prediction_stdv)).tolist()
lower = (prediction_mean - (1.96 * prediction_stdv)).tolist()
prediction_stdv = prediction_stdv.tolist()

for idx in range(sample):
    print(
        f"Prediction mean: {round(prediction_mean[idx][0], 2)}, "
        f"stddev: {round(prediction_stdv[idx][0], 2)}, "
        f"95% CI: [{round(upper[idx][0], 2)} - {round(lower[idx][0], 2)}]"
        f" - Actual: {targets[idx]}"
    )
